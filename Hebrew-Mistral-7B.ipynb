{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio numpy torch transformers nltk\n",
	"!pip install accelerate\n",
	"!pip install bitsandbytes\n",
    "!pip install typer==0.12\n",
    "!pip install spacy==3.7.4\n",
    "!pip install weasel==0.3.4"
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "model_name = \"yam-peleg/Hebrew-Mistral-7B\"\n",
    "cache_dir = \"hebrew_mistral_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, quantization_config=quantization_config)\n",
    "\n",
    "def generate_response(input_text, max_new_tokens, min_length, no_repeat_ngram_size, num_beams, early_stopping, temperature, top_p, top_k):\n",
    "   input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "   outputs = model.generate(\n",
    "       **input_ids,\n",
    "       max_new_tokens=max_new_tokens,\n",
    "       min_length=min_length,\n",
    "       no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "       num_beams=num_beams,\n",
    "       early_stopping=early_stopping,\n",
    "       temperature=temperature,\n",
    "       top_p=top_p,\n",
    "       top_k=top_k,\n",
    "       pad_token_id=tokenizer.eos_token_id,\n",
    "       do_sample=True\n",
    "   )\n",
    "   response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "   return response\n",
    "\n",
    "def create_paragraphs(bot_response, sentences_per_paragraph=4):\n",
    "   sentences = sent_tokenize(bot_response)\n",
    "   paragraphs = []\n",
    "   current_paragraph = \"\"\n",
    "\n",
    "   for i, sentence in enumerate(sentences, start=1):\n",
    "       current_paragraph += \" \" + sentence\n",
    "       if i % sentences_per_paragraph == 0:\n",
    "           paragraphs.append(current_paragraph.strip())\n",
    "           current_paragraph = \"\"\n",
    "\n",
    "   if current_paragraph:\n",
    "       paragraphs.append(current_paragraph.strip())\n",
    "\n",
    "   formatted_paragraphs = \"\\n\".join([f'<p style=\"text-align: right; direction: rtl;\">{p}</p>' for p in paragraphs])\n",
    "   return formatted_paragraphs\n",
    "\n",
    "def remove_paragraphs(text):\n",
    "   return text.replace(\"\\n\", \" \")\n",
    "\n",
    "def copy_last_response(history):\n",
    "    if history:\n",
    "        last_response = history[-1][1]\n",
    "        last_response = last_response.replace('<div style=\"text-align: right; direction: rtl;\">', '').replace('</div>', '')\n",
    "        last_response = last_response.replace('<p style=\"text-align: right; direction: rtl;\">', '').replace('</p>', '')\n",
    "        last_response = last_response.replace('\\n', ' ')\n",
    "        return last_response\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def chat(input_text, history, max_new_tokens, min_length, no_repeat_ngram_size, num_beams, early_stopping, temperature, top_p, top_k, create_paragraphs_enabled):\n",
    "   user_input = f'<div style=\"text-align: right; direction: rtl;\">{input_text}</div>'\n",
    "   response = generate_response(input_text, max_new_tokens, min_length, no_repeat_ngram_size, num_beams, early_stopping, temperature, top_p, top_k)\n",
    "\n",
    "   if create_paragraphs_enabled:\n",
    "       response = create_paragraphs(response)\n",
    "\n",
    "   bot_response = f'<div style=\"text-align: right; direction: rtl;\">{response}</div>'\n",
    "   history.append((user_input, bot_response))\n",
    "\n",
    "   return history, history, input_text\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "   gr.Markdown(\"# Hebrew-Mistral-7B Instract-bot\", elem_id=\"title\")\n",
    "   gr.Markdown(\"Model by Yam Peleg | GUI by Shmuel Ronen\", elem_id=\"subtitle\")\n",
    "   \n",
    "   chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "   \n",
    "   with gr.Row():\n",
    "       message = gr.Textbox(placeholder=\"Type your message...\", label=\"User\", elem_id=\"message\")\n",
    "       submit = gr.Button(\"Send\")\n",
    "\n",
    "   with gr.Row():\n",
    "       create_paragraphs_checkbox = gr.Checkbox(label=\"Create Paragraphs\", value=False)\n",
    "       remove_paragraphs_btn = gr.Button(\"Remove Paragraphs\")\n",
    "       copy_last_btn = gr.Button(\"Copy Last Response\")\n",
    "   \n",
    "   with gr.Accordion(\"Adjustments\", open=False):\n",
    "       with gr.Row():    \n",
    "           with gr.Column():\n",
    "               max_new_tokens = gr.Slider(minimum=10, maximum=1500, value=100, step=10, label=\"Max New Tokens\")\n",
    "               min_length = gr.Slider(minimum=10, maximum=300, value=100, step=10, label=\"Min Length\")\n",
    "               no_repeat_ngram_size = gr.Slider(minimum=1, maximum=6, value=4, step=1, label=\"No Repeat N-Gram Size\")\n",
    "           with gr.Column():\n",
    "               num_beams = gr.Slider(minimum=1, maximum=16, value=4, step=1, label=\"Num Beams\") \n",
    "               temperature = gr.Slider(minimum=0.1, maximum=2.0, value=0.2, step=0.1, label=\"Temperature\")\n",
    "               top_p = gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.1, label=\"Top P\")\n",
    "               top_k = gr.Slider(minimum=1, maximum=100, value=30, step=1, label=\"Top K\")\n",
    "       early_stopping = gr.Checkbox(value=True, label=\"Early Stopping\")\n",
    "   \n",
    "   submit.click(chat, inputs=[message, chatbot, max_new_tokens, min_length, no_repeat_ngram_size, num_beams, early_stopping, temperature, top_p, top_k, create_paragraphs_checkbox], outputs=[chatbot, chatbot, message])\n",
    "   remove_paragraphs_btn.click(remove_paragraphs, inputs=message, outputs=message)\n",
    "   copy_last_btn.click(copy_last_response, inputs=chatbot, outputs=message)\n",
    "   \n",
    "   demo.css = \"\"\"\n",
    "       #message, #message * {\n",
    "           text-align: right !important;\n",
    "           direction: rtl !important;\n",
    "       }\n",
    "       \n",
    "       #chatbot, #chatbot * {\n",
    "           text-align: right !important;\n",
    "           direction: rtl !important;\n",
    "       }\n",
    "       \n",
    "       #title, .label {\n",
    "           text-align: right !important;\n",
    "       }\n",
    "       \n",
    "       #subtitle {\n",
    "           text-align: left !important;\n",
    "       }\n",
    "   \"\"\"\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}